---
title: "homework"
author: "Xuechen Yan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

#hw0

## Question 1

The 1st example should contain texts and at least one figure

## Answer

```{r}
x = (1:20)
y = rnorm(20)#Generate 5 random numbers that obey the standard normal distribution
plot(x,y)#Draw a scatter plot of X and y
```

## Question 2

The 2nd example should contains texts and at least one table.

## Answer

```{r}
data("pressure");
names(pressure)
# knitr::kable(pressure)
print(pressure)
```

## Question 3

The 3rd example should contain at least a couple of LaTeX formulas.

## Answer

Let $Z$ be a bounded-mean random variable, i.e., $\mathbb{E}[|Z|]<\infty,$ on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, with cumulative distribution function $F(z)=\mathbb{P}(Z \leq z) .$ In this paper we interpret $Z$ as a cost. The value-at-risk (VaR) at confidence level $\alpha \in(0,1)$ is the $1-\alpha$ quantile of $Z,$ i.e., $\operatorname{VaR}_{\alpha}(Z)=$ $\min \{z \mid F(z) \geq 1-\alpha\} .$ The conditional value-at-risk $(\mathrm{CVaR})$ at confidence level $\alpha \in(0,1)$ is defined as:
$$
\operatorname{CVaR}_{\alpha}(Z)=\min _{w \in \mathbb{R}}\left\{w+\frac{1}{\alpha} \mathbb{E}\left[(Z-w)^{+}\right]\right\}
$$


#hw1

## Question

3.3 

## Answer

```{r}
y = runif(1000)#Generate 1000 random numbers evenly distributed in the [0, 1] interval
b = 2;a = 2
x = b/(1-y)^(1/a)#F(x)=1-(b/x)^a, x>=b>0, a>0
hist(x, prob = T)
z = seq(0, 50, .01)
lines(z,a*b^(a)/z^(a+1))#Superimpose the curve of the density function of Pareto(2,2)
```

## Question

3.9 and 3.10

## Answer

```{r}
n = 1
g = 1
for (n in 1:1000) {
  u1 = runif(1,-1,1)
  u2 = runif(1,-1,1)
  u3 = runif(1,-1,1)
  if(abs(u3)>abs(u2) & abs(u3)>abs(u1)){
    x[g] = u2
  }else{
    x[g] = u3
  }
  g=g+1
}#Generate iid U1, U2, U3 ∼ Uniform(−1, 1). If |U3|>=|U2| and |U3|>=|U1|, deliver U2, otherwise deliver U3.
y = x[1:1000]
hist(y, prob = T)
z = seq(-1,1,.01)
lines(z,(3/4)*(1-z^(2)))#Superimpose the curve of the density function of fe(x)
```

It can be seen from the above figure that the curve of the distribution of the density function given in 
$$
f_{e}(x)=\frac{3}{4}\left(1-x^{2}\right), \quad|x| \leq 1
$$
is basically consistent with that of the variable generated by the algorithm given in Exercise 3.9.

Therefore we can prove that the algorithm given in Exercise 3.9 generates variates from the density $f_{e}$.

## Question

3.13 

## Answer

```{r}
r = 4; beta = 2
gamma = rgamma(1000,r,beta)#Generate Λ ∼ Gamma(4,2)
y = rexp(1000,gamma)#Generate (Y|Λ=λ) ∼ fY(y|λ) = λ*e^(−λy)
hist(y,prob = T)
z = seq(0,15,.01)
lines(z,r*beta^r/(beta+z)^(r+1))#Superimpose the curve of the density function of F(y)=1-(β/(β+y))^r, y>=0
```

It can be seen from the above figure that the density histogram of the sample of the empirical and theoretical (Pareto) distributions is basically consistent with Pareto density curve.

So he mixture in Exercise 3.12 has a Pareto distribution with cdf 
$$F(y)=1-\left(\frac{\beta}{\beta+y}\right)^{r}, \quad y \geq 0$$

#hw2

## Question

5.1

## Answer

The exact value of the integral is
$$\int_{0}^{\pi / 3} \sin t d t =1-cos(\pi/3)=0.5$$
If we use Monte Carlo integration, then
$$\int_{0}^{\pi / 3} \sin t d t=(\pi/3)*E(\sin X), X\sim U(0,\pi/3)$$
```{r}
x=runif(1e4,0,pi/3)
beta_hat=mean(sin(x))*pi/3 
beta_hat
```

It can be seen from the above results that the result of a Monte Carlo estimate of $\int_{0}^{\pi / 3} \sin t dt$ is very close to the exact value of the integral.

## Question

5.7

## Answer

First we use simple Monte Carlo method and the antithetic variate approach to estimate $\theta=\int_{0}^{1} e^{x} d x$

```{r}
MC_theta=function(n,antithetic = TRUE){
  u=runif(n/2)
  if(!antithetic)
    v=runif(n/2)
  else
    v=1-u
  u=c(u,v)
  f=exp(u)
  theta_hat=mean(f)
}
n=1e3
set.seed(7)
MC1=MC_theta(n)#Monte Carlo estimate obtained by the antithetic variate approach
set.seed(7)
MC2=MC_theta(n,antithetic = F)#Monte Carlo estimate obtained by the simple Monte Carlo method
print(round(rbind(MC1,MC2),4))
```

Then we compute the theoretical value of the  integration.

```{r}
theta=integrate(function(x) exp(x),0,1)
theta
```

It can be seen from the above results that the result of a Monte Carlo estimate is very close to the exact value of the integral.
And the result obtained by the antithetic variate approach is more accurate.

Then we compute an empirical estimate of the percent reduction in variance using the antithetic variate.

```{r}
m=1e3
MC1=MC2=numeric(m)
for (i in 1:m) {
  MC1[i]=MC_theta(n)
  MC2[i]=MC_theta(n,antithetic = F)
}
sd1=sd(MC1)
sd2=sd(MC2)
c(sd1,sd2,sd1/sd2)
```

from the results above, we can find that the variance of the estimation is significantly reduced by the antithetic variate approach. The variance is reduced by about 80 percent.

## Question

5.11

## Answer

The variance of $c \hat{\theta}_{1}+(1-c) \hat{\theta}_{2}$ is
$$
\operatorname{Var}\left(\hat{\theta}_{2}\right)+c^{2} \operatorname{Var}\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)+2 c \operatorname{Cov}\left(\hat{\theta}_{2}, \hat{\theta}_{1}-\hat{\theta}_{2}\right)
$$
In the special case of antithetic variates, $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are identically distributed and $\operatorname{Cor}\left(\hat{\theta}_{1}, \hat{\theta}_{2}\right)=-1 .$ Then $\operatorname{Cov}\left(\hat{\theta}_{1}, \hat{\theta}_{2}\right)=-\operatorname{Var}\left(\hat{\theta}_{1}\right)$
and the variance is
$$
\operatorname{Var} \hat{\theta}_{c}=4 c^{2} \operatorname{Var}\left(\hat{\theta}_{1}\right)-4 c \operatorname{Var}\left(\hat{\theta}_{1}\right)+\operatorname{Var}\left(\hat{\theta}_{1}\right)=\left(4 c^{2}-4 c+1\right) \operatorname{Var}\left(\hat{\theta}_{1}\right)
$$
and the optimal constant is $c^{*}=1 / 2 .$ 

In general case, the minimum value of
$$
\operatorname{Var}\left(\hat{\theta}_{2}\right)+c^{2} \operatorname{Var}\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)+2 c \operatorname{Cov}\left(\hat{\theta}_{2}, \hat{\theta}_{1}-\hat{\theta}_{2}\right)
$$
is reached at
$$c^*=-\frac{cov(\hat \theta_{2},\hat \theta_{1}-\hat \theta_{2})}{Var(\hat \theta_{1}-\hat \theta_{2})}=-\frac{cov(\hat \theta_{2},\hat \theta_{1})-Var(\hat \theta_{2})}{Var(\hat \theta_{1})+Var(\hat \theta_{2})-2cov(\hat \theta_{1},\hat \theta_{2})}$$

#hw3

## Question

5.13

## Answer

Two importance functions:
$$f_{1}=\sqrt exe^{-x^{2}/2},\quad x>1$$
$$f_{2}=\frac{\sqrt e}{2} e^{-x/ 2}, \quad x>1$$
```{r}
n=1e4
x=seq(1,20,.01)
g=x^{2}/(2*pi)^(1/2)*exp(-x^(2)/2)
f1=x*exp((1-(x)^(2))/2)
f2=(1/2)*exp((1-x)/2)
plot(x,g,ylim=c(0,1.5),col='red',type='l')
lines(x,f1)
lines(x,f2,col='yellow')
```

From the picture above, $f_{1}$ should produce the smaller variance in estimating
$$
\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x
$$
by importance sampling. The reasons are as follows.

If $f(x)$ is the importance sampling distribution, and $X$ has pdf $f(x)$ supported on $A,$ then
$$
\theta=\int_{A} g(x) d x=\int_{A} \frac{g(x)}{f(x)} \phi(x) d x=E\left[\frac{g(X)}{f(X)}\right]
$$
If $X_{1}, \ldots, X_{n}$ is a random sample from the distribution of $X,$ the estimator is again the sample-mean
$$
\hat{\theta}=\overline{g(X)}=\frac{1}{n} \sum_{i=1}^{n} \frac{g\left(X_{i}\right)}{f\left(X_{i}\right)}
$$
Thus, the importance sampling method is a sample-mean method, and
$$
\operatorname{Var}(\hat{\theta})=E\left[\hat{\theta}^{2}\right]-(E[\hat{\theta}])^{2}=\int_{A} \frac{g^{2}(x)}{f(x)} d s-\theta^{2}
$$
The distribution of $X$ can be chosen to reduce the variance of the samplemean estimator. The minimum variance
$$
\left(\int_{A}|g(x)| d x\right)^{2}-\theta^{2}
$$
is obtained when
$$
f(x)=\frac{|g(x)|}{\int_{A}|g(x)| d x}
$$
Unfortunately, the problem is to estimate $\int_{A} g(x) d x,$ so it is unlikely that the value of $\int_{A}|g(x)| d x$ in the denominator of $f(x)$ is available. Although it may be difficult to choose $\phi(x)$ to attain minimum variance, variance may be "close to" optimal if $f(x)$ is chosen so that the shape of the density $f(x)$ is "close to" $|g(x)|$ on $A$.

## Question

5.15

## Answer

The stratified importance sampling estimate in Example 5.13
```{r}
N=1e4
k=5
M=100#number of times to repeat the estimation
est1=numeric(k)
est=numeric(M)
f=function(x)exp(-x)/(1+x^{2})#Integral function
g=function(x)exp(-x)/(1-exp(-1))
h=function(x)-log(1-x*(1-exp(-1))) 
for (i in 1:M) {
  for (j in 1:k) {
    x=h(runif(N/k,h((j-1)/k),h(j/k)))
    est1[j]=mean(f(x)/g(x))
  }
  est[i]=mean(est1)
}
est_sd=sd(est)
est=mean(est)
est;est_sd
```

True value
```{r}
theta=integrate(f,0,1)
theta
```

In Example 5.10 our best result was obtained with importance function $f_{3}(x)=$ $e^{-x} /\left(1-e^{-1}\right), 0<x<1 .$ From 10000 replicates we obtained the estimate $\hat{\theta}=0.5257801$ and an estimated standard error $0.0970314 .$ 

From the results of stratified importance sampling estimate, we can see that the difference between the estimated value and the true value has increased, but the variance of the estimation is significantly reduced. 

## Question

6.4

## Answer

Suppose that $X_{1}, \ldots, X_{n}$ are a random sample from a from a lognormal distribution with unknown parameters.

Then, $y_{i}=lnX_{i}\sim N(\mu,\sigma^{2})$. And $t=\frac{\sqrt n(\bar y-\mu)}{s}\sim t_{n-1}$, where $\bar y=\frac{1}{n}\sum_{i=1}^{n}y_{i}$, and $s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(y_{i}-\bar y)^{2}$

Therefore, a one-side 95% confidence interval for
the parameter $\mu$ is 
$$\left(-\infty, \bar y+ t_{n-1}(0.95)s/\sqrt n \right]$$

Next, we use a Monte Carlo method to obtain an empirical estimate of the confidence level. The steps are as follows:

1. For each replicate, indexed $j=1, \ldots, m:$

(a) Generate the $j^{t h}$ random sample, $X_{1}^{(j)}, \ldots, X_{n}^{(j)}$

(b) Compute the confidence interval $C_{j}$ for the $j^{t h}$ sample.

(c) Compute $y_{j}=I\left(\theta \in C_{j}\right)$ for the $j^{t h}$ sample.

2. Compute the empirical confidence level $\bar{y}=\frac{1}{m} \sum_{j=1}^{m} y_{j}$

In this example we suppose $\mu=0, \sigma=1, n=20$, $m=1000$ replicates, and $\alpha=0.05 .$ The sample proportion of intervals that


```{r}
n=20
alpha=.05
m=1e3
set.seed(7)
UCL = replicate(m, expr = {
x=log(rlnorm(n,0,1))
mean(x)+qt(1-alpha,n-1)*var(x)^(1/2) / n^(1/2)
} )
set.seed(7)
sum(0<UCL)
mean(0<UCL)
```

We can see from the results that an empirical estimate of the confidence level obtained by Monte Carlo method is very closed to the true level.

## Question

6.5

## Answer

Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of
$\chi^{2}(2)$ data with sample size $n = 20$.
```{r}
n=20
alpha=.05
m=1e3
set.seed(7)
UCL = replicate(m, expr = {
x=rchisq(n,2)
mean(x)+qt(1-alpha,n-1)*var(x)^(1/2) / n^(1/2)
} )
set.seed(7)
print(round(c(sum(0<UCL),sum(1<UCL),sum(2<UCL)),3))
print(round(c(mean(0<UCL),mean(1<UCL),mean(2<UCL)),3))
```

From the results, we can see that when $\mu$ rises to 2, there are 901 or 90.1% of the intervals contained the population mean. Compare the t-interval results with the simulation results in Example 6.4. We can see that the t-interval is more robust to departures from normality than the interval for variance.

#hw4

## Question

6.7 Estimate the power of the skewness test of normality against symmetric $\operatorname{Beta}(\alpha, \alpha)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(\nu) ?$

## Answer

Estimate the power of the skewness test of normality against symmetric $\operatorname{Beta}(\alpha, \alpha)$ distributions

```{r}
set.seed(7)
sk=function(x){
  #computes the sample skewness coeff
  xbar=mean(x)
  m3=mean((x-xbar)^3)
  m2=mean((x-xbar)^2)
  return(m3/m2^1.5 )
}

alpha=.05
n=20
m=1e3
a=seq(1,30,.2)
M=length(a)
power=numeric(M)
cv=qnorm(1-alpha/2,0,sqrt(6*(n-2)/((n+1)*(n+3))))

for (i in 1:M) {
  sktests=replicate(m,expr = {
    x=rbeta(n,a[i],a[i])
    sktests=as.integer(abs(sk(x))>=cv)
    sktests
    })
  power[i]=mean(sktests)
  se = sqrt(power*(1-power)/m)
}

plot(a,power,xlab = 'a',ylab = 'power',ylim = c(0,.06),type = 'o',pch=20)
lines(a,power+se,lty = 5,col='blue')
lines(a,power-se,lty = 5,col='blue')
abline(h=.05,lty = 5,col='red')
```
Most power of the skewness test of normality against symmetric Beta(a,a) distribution is under 0.05. And the empirical power increases with the increase of a. When a is near 0, the empirical power is far less than 0.05. When a exceeds 20,
it fluctuates around 0.05.

Then we stimate the power of the skewness test of normality against symmetric $t(\nu) $distributions

```{r}
set.seed(7)
sk=function(x){
  #computes the sample skewness coeff
  xbar=mean(x)
  m3=mean((x-xbar)^3)
  m2=mean((x-xbar)^2)
  return(m3/m2^1.5 )
}

alpha=.05
n=20
m=1e3
a=seq(1,30,.2)
M=length(a)
power=numeric(M)
cv=qnorm(1-alpha/2,0,sqrt(6*(n-2)/((n+1)*(n+3))))

for (i in 1:M) {
  sktests=replicate(m,expr = {
    x=rt(n,a[i])
    sktests=as.integer(abs(sk(x))>=cv)
    sktests
    })
  power[i]=mean(sktests)
  se = sqrt(power*(1-power)/m)
}

plot(a,power,xlab = 'a',ylab = 'power',ylim = c(0,1),type = 'o',pch=20)
lines(a,power+se,col='blue',lty = 5)
lines(a,power-se,col='blue',lty = 5)
abline(h=.05,col='red',lty = 5)
```

Comparing the above two figures, we can find that the results are different for heavy-tailed symmetric alternatives. For t distribution, the empirical power is bigger than 0.1 and it decreases to 0.1 with the increase of v.

## Question

6.8 Refer to Example $6.16.$ Repeat the simulation, but also compute the $F$ test of equal variance, at significance level $\hat{\alpha} \doteq 0.055 .$ Compare the power of the Count Five test and $F$ test for small, medium, and large sample sizes. (Recall that the $F$ test is not applicable for non-normal distributions.)

## Answer

Refer to Example 6.16 for small, medium, and large sample sizes..
```{r}
set.seed(1)
count5test=function(x, y) {
  X=x - mean(x)
  Y=y - mean(y)
  outx=sum(X > max(Y)) + sum(X < min(Y))
  outy=sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer(max(c(outx, outy)) > 5))
}
sigma1=1
sigma2=1.5
m=1e4
n=c(20,200,2000)
power=numeric(length(n))
for (i in 1:3) {
  power[i]=mean(replicate(m, expr={
    x=rnorm(n[i], 0, sigma1)
    y=rnorm(n[i], 0, sigma2)
    count5test(x, y)
}))
}
result=rbind(n,power)
print(result)
```



Compute the $F$ test of equal variance, at significance level $\hat{\alpha} \doteq 0.055 .$

```{r}
set.seed(3)
sigma1=1
sigma2=1.5
alpha=.055
m=1e4
n=c(20,200,2000)
power=numeric(length(n))
for (i in 1:3) {
  pvalues=(replicate(m, expr={
    x=rnorm(n[i], 0, sigma1)
    y=rnorm(n[i], 0, sigma2)
    Ftest=var.test(x,y,alternative = 'two.sided',conf.level = 1-alpha)
    Ftest$p.value
}))
  power[i]=mean(pvalues<=alpha)
}
result=rbind(n,power)
print(result)
```

As can be seen from the above results, whether in small, medium, and large sample sizes, the power of samples in F-test is larger than that in Count Five test. In other words, in F-test, the probability of samples falling into the rejection region is higher, which is correspond to reality. Besides, in the case of small sample size, the effect of the two tests is not ideal. When the sample size reaches 200, the effect of both tests is very good.

## Question

6.C Repeat Examples 6.8 and 6.10 for Mardia's multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X$ and $Y$ are iid, the multivariate population skewness $\beta_{1, d}$ is defined by Mardia as
$$
\beta_{1, d}=E\left[(X-\mu)^{T} \Sigma^{-1}(Y-\mu)\right]^{3}
$$
Under normality, $\beta_{1, d}=0 .$ The multivariate skewness statistic is
$$
b_{1, d}=\frac{1}{n^{2}} \sum_{i, j=1}^{n}\left(\left(X_{i}-\bar{X}\right)^{T} \widehat{\Sigma}^{-1}\left(X_{j}-\bar{X}\right)\right)^{3}
$$
where $\hat{\Sigma}$ is the maximum likelihood estimator of covariance. Large values of $b_{1, d}$ are significant. The asymptotic distribution of $n b_{1, d} / 6$ is chisquared with $d(d+1)(d+2) / 6$ degrees of freedom.

## Answer

Letting d = 2, repeat Examples 6.8 (Skewness test of normality):
```{r}
set.seed(4)
sk=function(n,x_y,Sigma_hat){
  #computes the sample skewness coeff
  m1=numeric(n^(2))
  for (i in 1:n) {
    a=matrix(c(x_y[i,1]-mean(x_y[,1]),x_y[i,2]-mean(x_y[,2])),2,1)
    for (j in 1:n) {
      b=matrix(c(x_y[j,1]-mean(x_y[,1]),x_y[j,2]-mean(x_y[,2])),2,1)
      m1[(i-1)*n+j]=(t(a)%*%solve(Sigma_hat)%*%b)^(3)
  }
  }
  sk=1/n^(2)*sum(m1)
  return(sk)
}

alpha=.05
n=c(10, 20, 30, 50, 100)#sample sizes
m=500
d=2
N=length(n)
p.reject=numeric(N)
#critical value for the skewness test
cv1=qchisq(1-alpha/2,d*(d+1)*(d+2)/6)
cv2=qchisq(alpha/2,d*(d+1)*(d+2)/6)
#Parameters of multidimensional normal distribution
mu=matrix(c(0,0),2,1)
Sigma=matrix(c(1,0,0,1),2,2)#X and Y are iid

for (i in 1:N) {
  sktests=numeric(m)
  for (j in 1:m) {
    library(MASS)
    x_y=mvrnorm(n[i],mu,Sigma)
    u=vector('list',n[i])
    for (h in 1:n[i]) {
      z=matrix(c(x_y[h,1]-mean(x_y[,1]),x_y[h,2]-mean(x_y[,2])),2,1)
      u[[h]]=z%*%t(z)
    }
    Sigma_hat=1/n[i]*Reduce('+',u)
    sktests[j]=as.integer(n[i]*sk(n[i],x_y,Sigma_hat)/6<=cv2|n[i]*sk(n[i],x_y,Sigma_hat)/6>=cv1)
  }
  p.reject[i]=mean(sktests) #proportion rejected
}
print(rbind(n,p.reject))
```
The results of the simulation are the empirical estimates of Type I error rate. These estimates are closer to the nominal level α = 0.05.

Repeat Examples 6.10 (Power of the skewness test of normality):

```{r}
alpha=.1
n=30
m=500
epsilon=c(seq(0, .15, .01), seq(.15, 1, .05))
N=length(epsilon)
pwr=numeric(N)
mu=matrix(c(0,0),2,1)
#critical value for the skewness test
cv1=qchisq(1-alpha/2,d*(d+1)*(d+2)/6)
cv2=qchisq(alpha/2,d*(d+1)*(d+2)/6)

for (j in 1:N) { #for each epsilon
  e=epsilon[j]
  sktests=numeric(m)
  for (i in 1:m) { #for each replicate
    x=matrix(nrow = n,ncol = 2)
    for (h in 1:n) {
      a=sample(c(1,10), replace = TRUE,size = 1, prob = c(1-e, e))
      Sigma=matrix(c(a,0,0,a),2,2)
      library(MASS)
      x[h,]=mvrnorm(1,mu,Sigma)
    }
    u=vector('list',n)
    for (w in 1:n) {
      z=matrix(c(x[w,1]-mean(x[,1]),x[w,2]-mean(x[,2])),2,1)
      u[[w]]=z%*%t(z)
    }
    Sigma_hat=1/n*Reduce('+',u)
    sktests[i]=as.integer(n*sk(n,x,Sigma_hat)/6<=cv2|n*sk(n,x,Sigma_hat)/6>=cv1)
  }
  pwr[j] <- mean(sktests)
}
plot(epsilon,pwr,xlab = 'ε',ylab = 'power',ylim = c(0,1),type = 'o',pch=20)
abline(h=.1,col='red',lty = 5)
```

The empirical power curve is shown in figure above. Note that the power curve crosses the horizontal line corresponding to α = 0.10 at both endpoints, ε = 0 and ε = 1 where the alternative is Multivariate normal distribution. For 0 < ε < 1, the empirical power of the test is greater than 0.10 and highest when ε is about 0.15.

## Question

Discussion
(1)What is the corresponding hypothesis test problem?

(2)What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test?

(3)What information is needed to test your hypothesis?

## Answer

(1)Let$d_{i}=power1_{i}-power2{i}$, then the hypothesis test question is
$$H_{1}:\bar d=0 \leftrightarrow \bar d\neq 0$$

(2)We should use paired-t test. Because this problem is to study the difference between the results of two different treatments given to the same research object. In other words, the power of the two methods comes from the same set of objects.

(3)First, we assume that $power1$ and $power2$ obey the normal distribution. Then we should calculate n sample values of $power1$ and $power2$ respectively, and calculate
$d_{i}=power1_{i}-power2{i}$. So we can get the mean$\bar d$ and  standard deviation $s_{d}$, thereby the statistics t can be calculated. Finally, we can use this statistic to make statistical inference.


#hw5

## Question

7.1 Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

## Answer

Compute a jackknife estimate of the bias and the standard error 
```{r}
set.seed(7)
library(bootstrap)
law=law
n=nrow(law)
theta_hat=cor(law$LSAT,law$GPA)
theta_jack=numeric(n)
for (i in 1:n) {
  x=law$LSAT[-i]
  y=law$GPA[-i]
  theta_jack[i]=cor(x,y)
}
bias=(n-1)*(mean(theta_jack)-theta_hat)
se=sqrt((n-1)/n*sum((theta_jack-mean(theta_jack))^2))
print(list(est=theta_hat,bias_jack=bias,se_jack=se,bias_boot=-0.0048,se_boot=0.1358))
```

## Question

7.5 Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures $1/\lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

## Answer

#### Compute 95% bootstrap confidence intervals

First, we compute 95% bootstrap confidence intervals for the mean time between failures $1/\lambda$ by the standard normal, basic, percentile

Let $\theta = 1/\lambda$Using first-order moment estimation we can get that $\hat \theta = \bar x$

```{r}
data=matrix(c(3,5,7,18,43,85,91,98,100,130,230,487),12,1)
theta_boot=function(data,i){
  a=data[i,1]
  theta_boot=mean(a)
}
library(boot)
boot_obj=boot(data,statistic = theta_boot,R = 1e3)
print(boot.ci(boot_obj,cof=0.95,type = c('basic','norm','perc')))
```

Then, we compute 95% bootstrap confidence intervals for the mean time between failures $1/\lambda$ by the BCa methods

```{r}
boot.BCa=function(x, th0, th, stat, conf = .95) {
# bootstrap with BCa bootstrap confidence interval
# th0 is the observed statistic
# th is the vector of bootstrap replicates
# stat is the function to compute the statistic
n=nrow(x) #observations in rows
alpha=(1 + c(-conf, conf))/2
zalpha=qnorm(alpha)
# the bias correction factor
z0=qnorm(sum(th < th0) / length(th))
# the acceleration factor (jackknife est.)
th.jack=numeric(n)
for (i in 1:n) {
  th.jack[i]=mean(x[-i, ])
}
L=mean(th.jack) - th.jack
a=sum(L^3)/(6 * sum(L^2)^1.5)
# BCa conf. limits
adj.alpha=pnorm(z0 + (z0+zalpha)/(1-a*(z0+zalpha)))
limits=quantile(th, adj.alpha, type=6)
return(list("est"=th0, "BCa"=limits))
}
B=1e3
theta_boot=numeric(B)
n=nrow(data)
a=numeric(n)
for (j in 1:B) {
  h=sample(1:n,n,replace = T)
  a=data[h]
  theta_boot[j]=mean(a)
}
theta_hat=mean(data)
#compute the BCa interval
boot.BCa(data,theta_hat,theta_boot,stat,conf = .95)

```

#### Why they may differ

(1) When we use a standard normal bootstrap confidence interval, we suppose that $\hat{\theta}$ is approximately normal with mean $\theta$ and constant variance $\sigma^{2}(\hat{\theta})$ that does not depend on the parameter $\theta .$ However, under this circumstance, $\sigma^{2}(\hat{\theta})=\frac{1}{n\lambda^{2}}$ don't have constant variance with respect to the target parameter. And BCa methods take this into account, The acceleration factor aims to adjust the confidence limits to account for the possibility that the variance of the estimator may depend on the true value of the target parameter.

(2) A bootstrap percentile interval uses the empirical distribution of the bootstrap replicates as the reference distribution. The quantiles of the empirical distribution are estimators of the quantiles of the sampling distribution of $\hat{\theta}$, so that these (random) quantiles may match the true distribution better when the distribution of $\hat{\theta}$ is not normal. 

(3) The percentile interval has some
theoretical advantages over the standard normal interval and somewhat better.

(4) BCa is a modified version of percentile intervals that have better theoretical properties and better performance in practice.

## Question

7.8 Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat \theta$.

## Answer

```{r}
set.seed(7)
library(bootstrap)
cov1=cov(scor)
lambda_hat=eigen(cov1)$values
theta_hat=lambda_hat[1]/sum(lambda_hat)
n=nrow(scor)
theta_jack=numeric(n)
for (i in 1:n) {
  scor_jack=scor[-i,]
  cov_jack=cov(scor_jack)
  lambda_jack=eigen(cov_jack)$values
  theta_jack[i]=lambda_jack[1]/sum(lambda_jack)
}
bias=(n-1)*(mean(theta_jack)-theta_hat)
se=sqrt((n-1)/n*sum((theta_jack-mean(theta_jack))^2))
print(list(est=theta_hat,bias_jack=bias,se_jack=se))
```

## Question

7.11 In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

## Answer

```{r}
library(DAAG)
chemical=ironslag$chemical
magnetic=ironslag$magnetic
n=length(magnetic)
e1=e2=e3=e4=numeric(n-1)
set.seed(7)
for (i in 1:n-1) {
  x=chemical[-c(i,i+1)]
  y=magnetic[-c(i,i+1)]
  
  J1=lm(y~x)
  y1=J1$coef[1]+J1$coef[2]*chemical[c(i,i+1)]
  e1[i]=mean(abs(y1-magnetic[c(i,i+1)]))
  
  J2=lm(y~x+I(x^2))
  y2=J2$coef[1]+J2$coef[2]*chemical[c(i,i+1)]+J2$coef[3]*chemical[c(i,i+1)]^2
  e2[i]=mean(abs(y2-magnetic[c(i,i+1)]))
  
  J3=lm(log(y)~x)
  logy3=J3$coef[1]+J3$coef[2]*chemical[c(i,i+1)]
  y3=exp(logy3)
  e3[i]=mean(abs(y3-magnetic[c(i,i+1)]))
  
  J4=lm(log(y)~log(x))
  logy4=J4$coef[1]+J4$coef[2]*log(chemical[c(i,i+1)])
  y4=exp(logy4)
  e4[i]=mean(abs(y4-magnetic[c(i,i+1)]))
}
c(mean(e1^2),mean(e2^2),mean(e3^2),mean(e4^2))
```

According to the prediction error criterion, Model 2, the quadratic model, would be the best fit for the data.

```{r}
L=lm(magnetic ~ chemical + I(chemical^2))
L
```

The fitted regression equation for Model 2 is

$$
\hat{Y}=24.49262-1.39334 X+0.05452 X^{2}
$$

#hw6

## Question

8.3 The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

## Answer

```{r}
# Count 5 test
count5_test=function(x,y){
  X=x-mean(x)
  Y=y-mean(y)
  outx=sum(X>max(Y))+sum(X<min(Y))
  outy=sum(Y>max(X))+sum(Y<min(X))
  return(as.integer(max(c(outx,outy))>5))
}
R=1e3
# Permutation test
permutation_test=function(n1,z,R){
  out=numeric(R)
  n=length(z)
  for (r in 1:R) {
    k=sample(1:n,n,replace = F)
    a=z[k]
    x=a[1:n1]
    y=a[-(1:n1)]
    X=x-mean(x)
    Y=y-mean(y)
    outx=sum(X>max(Y))+sum(X<min(Y))
    outy=sum(Y>max(X))+sum(Y<min(X))
    out[r]=as.integer(max(c(outx,outy))>5)
  }
  sum(out)/R
}

n1=10;n2=20# sample sizes
m=1e3
out1=mean(replicate(m,expr={
  x=rnorm(n1)
  y=rnorm(n2)
  count5_test(x,y)
}))
out2=mean(replicate(m,expr = {
  x=rnorm(n1)
  y=rnorm(n2)
  z=c(x,y)
  permutation_test(n1,z,1e3)
})<0.1)
print(round(c(count5_test=out1,permutation_test=out2),3))
```

## Question

Design experiments for evaluating the performance of the NN, energy, and ball methods in various situations.

## Answer

(1) Unequal variances and equal expectations

```{r}
library(RANN)
library(MASS)
library(boot)
library(Ball)
library(energy)
Tn=function(z,ix,sizes,k){
  n1=sizes[1];n2=sizes[2];n=n1+n2
  z=z[ix,]
  NN=nn2(data=z,k=k+1)
  block1=NN$nn.idx[1:n1,-1]
  block2=NN$nn.idx[(n1+1):n,-1]
  i1=sum(block1<n1+.5)
  i2=sum(block2>n1+.5)
  (i1+i2)/(k*n)
}
n1=n2=50
n=n1+n2
N=c(n1,n2)
R=999
m=1e3
k=3
p=2
eqdist.nn=function(z,sizes,k){
  boot.obj=boot(data = z,statistic = Tn,R=R,sim = "permutation",sizes=sizes,k=k)
  ts=c(boot.obj$t0,boot.obj$t)
  p.value=mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}
p.values1=matrix(nrow=m,ncol=3)
for (i in 1:m) {
  x=matrix(rnorm(n1*p),ncol=p)
  y=cbind(rnorm(n2,0,.5),rnorm(n2))
  z=rbind(x,y)
  p.values1[i,1]=eqdist.nn(z,N,k)$p.value#NN test
  p.values1[i,2]=eqdist.etest(z,sizes = N,R=R)$p.value# energy test
  p.values1[i,3]=bd.test(x=x,y=y,R=R,seed=i*12345)$p.value# Ball test
}
alpha=0.1;
pow1=colMeans(p.values1<alpha)
pow1

```

(2) Unequal variances and unequal expectations

```{r}
p.values2=matrix(nrow=m,ncol=3)
for (i in 1:m) {
  x=matrix(rnorm(n1*p),ncol=p);
  y=cbind(rnorm(n2),rnorm(n2,0.5,1.5));
  z=rbind(x,y)
  p.values2[i,1]=eqdist.nn(z,N,k)$p.value
  p.values2[i,2]=eqdist.etest(z,sizes = N,R=R)$p.value
  p.values2[i,3]=bd.test(x=x,y=y,R=R,,seed=i*12345)$p.value
}
alpha=0.1;
pow2=colMeans(p.values2<alpha)
pow2
```

As can be seen from the above results, ball method could be more powerful for non-location family distribution. When the variance is different and the expectation is the same, the effect of the energy method is the worst. But when the expectation and variance are both different, the effect of the NN method is the worst.

(3) Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)

```{r}
p.values3=matrix(nrow=m,ncol=3)
for (i in 1:m) {
  lambda=0.5
  x=as.vector(rt(n1,1))#t distribution with 1 df
  y=as.vector(lambda*rnorm(n2)+(1-lambda)*rnorm(n2,0,2))#bimodel distribution
  z=c(x,y)
  o=rep(0,length(z))
  z=as.data.frame(cbind(z,o))
  p.values3[i,1]=eqdist.nn(z,N,k)$p.value
  p.values3[i,2]=eqdist.etest(z,sizes = N,R=R)$p.value
  p.values3[i,3]=bd.test(x=x,y=y,R=R,seed=i*12345)$p.value
}
alpha=0.1;
pow3=colMeans(p.values3<alpha)
pow3
```

As can be seen from the above results, When the distribution is different, energy method could be more powerful.

(4) Unbalanced samples

```{r}
library(RANN)
library(MASS)
library(boot)
library(Ball)
library(energy)
Tn=function(z,ix,sizes,k){
  n1=sizes[1];n2=sizes[2];n=n1+n2
  z=z[ix,]
  NN=nn2(data=z,k=k+1)
  block1=NN$nn.idx[1:n1,-1]
  block2=NN$nn.idx[(n1+1):n,-1]
  i1=sum(block1<n1+.5)
  i2=sum(block2>n1+.5)
  (i1+i2)/(k*n)
}
n1=10;n2=100# Different sample sizes
n=n1+n2
N=c(n1,n2)
R=999
m=1e3
k=3
p=2
eqdist.nn=function(z,sizes,k){
  boot.obj=boot(data = z,statistic = Tn,R=R,sim = "permutation",sizes=sizes,k=k)
  ts=c(boot.obj$t0,boot.obj$t)
  p.value=mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}
p.values4=matrix(nrow=m,ncol=3)
for (i in 1:m) {
  x=matrix(rnorm(n1*p),ncol=p)
  y=cbind(rnorm(n2),rnorm(n2,0.5,1.5))
  z=rbind(x,y)
  p.values4[i,1]=eqdist.nn(z,N,k)$p.value
  p.values4[i,2]=eqdist.etest(z,sizes = N,R=R)$p.value
  p.values4[i,3]=bd.test(x=x,y=y,R=R,seed=i*12345)$p.value
}
alpha=0.1;
pow4=colMeans(p.values4<alpha)
pow4
```

Comparing the above results with the results of (2), we can find that the accuracy of the test will decrease significantly when the samples are unbalanced. 

#hw7

## Question

9.4 Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

## Answer

```{r}
dLaplace=function(x){
  y=0.5*exp(-abs(x))
  return(y)
}
Metropolis=function(n,x0,sigma){
  x=numeric(n)
  x[1]=x0
  u=runif(n)
  k=0
  for (i in 2:n) {
    y=rnorm(1,x[i-1],sigma)
    if(u[i]<=(dLaplace(y)/dLaplace(x[i-1])))
      x[i]=y
      else{
        x[i]=x[i-1]
        k=k+1
      }
  }
  return(list(x=x,k=k))
}

n=2e3
sigma=c(.05,.3,2,16)
x0=25
rw1=Metropolis(n,x0,sigma[1])
rw2=Metropolis(n,x0,sigma[2])
rw3=Metropolis(n,x0,sigma[3])
rw4=Metropolis(n,x0,sigma[4])

print(c(rw1$k/n,rw2$k/n,rw3$k/n,rw4$k/n))
```

From the results above, we can see that only the third chain has a rejection rate in the range [0.15, 0.5].

Then we compare the chains generated when different variances are used for the proposal distribution.

```{r}
plot(rw1$x,type = 'l',ylab = "x",xlab = "sigma=0.05")
plot(rw2$x,type = 'l',ylab = "x",xlab = "sigma=0.5")
abline(h=2.5,lty = 1,col='blue')
abline(h=-2.5,lty = 1,col='blue')
plot(rw3$x,type = 'l',ylab = "x",xlab = "sigma=2")
abline(h=2.5,lty = 1,col='blue')
abline(h=-2.5,lty = ,col='blue')
plot(rw4$x,type = 'l',ylab = "x",xlab = "sigma=16")
abline(h=2.5,lty = 1,col='blue')
abline(h=-2.5,lty = 1,col='blue')
```

In the first plot of above figure with $\sigma=0.05,$ the ratios $r\left(X_{t}, Y\right)$ tend to be large and almost every candidate point is accepted. The increments are small and the chain is almost like a true random walk. Chain 1 has not converged to the target in 2000 iterations. The chain in the second plot generated with $\sigma=0.3$ is converging very slowly and requires a much longer burn-in period. In the third plot $(\sigma=2)$ the chain is mixing well and converging to the target distribution after a short burn-in period of about $500.$ Finally, in the fourth plot, where $\sigma=16$, the ratios $r\left(X_{t}, Y\right)$ are smaller and most of the candidate points are rejected. The fourth chain converges, but it is inefficient.



## Question

For Exercise 9.4, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat R<1.2$. 

## Answer

```{r}
G_R=function(psi){
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi=as.matrix(psi)
  n=ncol(psi)
  k=nrow(psi)
  psi_mean=rowMeans(psi) #row means
  B=n*var(psi_mean) #between variance est.
  psi_w=apply(psi, 1, "var") #within variances
  W=mean(psi_w) #within est.
  v_hat=W*(n-1)/n+(B/n) #upper variance est
  r_hat=v_hat/W #G-R statistic
  return(r_hat)
}

dLaplace=function(x){
  y=0.5*exp(-abs(x))
  return(y)
}

Laplace_chain=function(N,X1,sigma){
  x=numeric(N)
  x[1]=X1
  u=runif(N)
  for (i in 2:N) {
    y=rnorm(1,x[i-1],sigma)
    if(u[i]<=(dLaplace(y)/dLaplace(x[i-1])))
      x[i]=y
      else{
        x[i]=x[i-1]
      }
  }
  return(x)
}


k=4 #number of chains to generate
n=15000 #length of chains
b=1000 #burn-in length

#choose overdispersed initial values
x0=c(10,20,30,40)

rhat=matrix(0,nrow = 4,ncol = n)

#sigma=0.05
sigma1=.05
X=matrix(0, nrow=k, ncol=n)
for (i in 1:k)
  X[i, ]=Laplace_chain(n,x0[i],sigma1)
#compute diagnostic statistics
psi=t(apply(X, 1, cumsum))
for (j in 1:nrow(psi))
  psi[j,]=psi[j,] / (1:ncol(psi))
for (h in (b+1):n)
  rhat[1,h]=G_R(psi[,1:h])

#sigma=0.3
sigma1=.3
X=matrix(0, nrow=k, ncol=n)
for (i in 1:k)
  X[i, ]=Laplace_chain(n,x0[i],sigma1)
psi=t(apply(X, 1, cumsum))
for (j in 1:nrow(psi))
  psi[j,]=psi[j,] / (1:ncol(psi))
for (h in (b+1):n)
  rhat[2,h]=G_R(psi[,1:h])

#sigma=2
sigma1=2
X=matrix(0, nrow=k, ncol=n)
for (i in 1:k)
  X[i, ]=Laplace_chain(n,x0[i],sigma1)
psi=t(apply(X, 1, cumsum))
for (j in 1:nrow(psi))
  psi[j,]=psi[j,] / (1:ncol(psi))
for (h in (b+1):n)
  rhat[3,h]=G_R(psi[,1:h])

#sigma=16
sigma1=16
X=matrix(0, nrow=k, ncol=n)
for (i in 1:k)
  X[i, ]=Laplace_chain(n,x0[i],sigma1)
psi=t(apply(X, 1, cumsum))
for (j in 1:nrow(psi))
  psi[j,]=psi[j,] / (1:ncol(psi))
for (h in (b+1):n)
  rhat[4,h]=G_R(psi[,1:h])
  

#plot the sequence of R-hat statistics
for (i in 1:4) {
  plot(rhat[i,(b+1):n], type="l", xlab="", ylab="R")
  abline(h=1.2, lty=2)
}
  
```

The result of the above figure is consistent with that of Exercise 9.4. Chain 1 almost does not converge. Chain 2 is converging very slowly and requires a much longer burn-in period. Chain 3 is mixing well and converging to the target distribution after a short burn-in period of about $500.$ The fourth chain converges, but it is inefficient.

## Question

11.4 Find the intersection points $A(k)$ in $(0, \sqrt{k})$ of the curves
$$
S_{k-1}(a)=P\left(t(k-1)>\sqrt{\frac{a^{2}(k-1)}{k-a^{2}}}\right)
$$
and
$$
S_{k}(a)=P\left(t(k)>\sqrt{\frac{a^{2} k}{k+1-a^{2}}}\right)
$$
k=c(4,25,100,
for k = 4 : 25, 100, 500, 1000, where t(k) is a Student t random variable with k degrees of freedom. (These intersection points determine the critical values for a t-test for scale-mixture errors proposed by Sz´ekely [260].)

## Answer

```{r}
S_a=function(k,a){
 q=sqrt(a^2*k/(k+1-a^2))
 pt(q,df=k,lower.tail=F)
}
solve=function(k){
  output=uniroot(function(a){S_a(k,a)-S_a(k-1,a)},lower=0.5,upper=2)
  output$root
}
k=c(4,25,100,500,1000)
root=numeric(length(k))
for (i in 1:5) {
  root[i]=round(solve(k[i]),4)
}
print(rbind(k,root))
```

#hw8

## Question

A-B-O blood type problem

## Answer

```{r}
#EM
eval_f=function(x,y,nA=444,nB=132,nOO=361,nAB=63) {
  a=1-sum(y)
  nAA=nA*y[1]^2/(y[1]^2+2*y[1]*a)
  nBB=nB*y[2]^2/(y[2]^2+2*y[2]*a)
  b=1-sum(x)
  return(-2*nAA*log(x[1])-2*nBB*log(x[2])-2*nOO*log(b)-(nA-nAA)*log(2*x[1]*b)-(nB-nBB)*log(2*x[2]*b)-nAB*log(2*x[1]*x[2]))
}

library(nloptr)
opts = list("algorithm"="NLOPT_LN_COBYLA",
             "xtol_rel"=1e-10)
mle=1455
b=matrix(c(0,0.3,0,0.15),2,2)#Initial value
j = 2
while (sum(abs(b[j,]-b[j-1,]))>1e-6) {
res=nloptr( x0=c(0.3,0.15),
            eval_f=eval_f,
            lb=c(0,0),ub=c(1,1), 
            opts=opts,
            y=b[j,],
            nA=444,nB=132,nOO=361,nAB=63)
j=j+1
b=rbind(b,res$solution)
mle=c(mle,eval_f(x=b[j,],y=b[j-1,]))
}
#the values of p and q that maximize the conditional
b
#the corresponding log-maximum likelihood values
mle
```

As can be seen from the results, the the corresponding log-maximum likelihood values are decreasing

## Question

3(P204). Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:

## Answer

```{r echo=TRUE, warning=FALSE}
attach(mtcars)
formulas = list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)

#With loops
out1=vector("list", length(formulas))
for (i in seq_along(formulas)){
  out1[[i]]=lm(formulas[[i]],mtcars)
}
out1

#With lapply
out2=vector("list", length(formulas))
out2=lapply(formulas,function(x) {lm(x,mtcars)})
out2
```


## Question

3(P214). The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.

## Answer

```{r}
trials=replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
sapply(trials,function(p) {p[["p.value"]]})
```


## Question

6. Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

## Answer

```{r}
x=list(a = 1:10, beta = exp(-3:3), logic=c(TRUE,FALSE,FALSE,TRUE))
lapply(x,function(x) vapply(x, quantile, c(1,2,5,6,8)))

#combination of Map and vapply
mv=function(X,f,f_value){
  out=Map(function(x) vapply(x,f,f_value), X)
  out
}
mv(x,quantile,c(1,2,5,6,8))
```

#hw9

## Question

Write an Rcpp function for  Exercise 9.4 (page 277, Statistical Computing with R.

1.Compare the corresponding generated random numbers with those by the R function you wrote before using the function “qqplot”.

2.Campare the computation time of the two functions with the function “microbenchmark”.

3.Comments your results.

## Answer

(1) Compare the corresponding generated random numbers with those by the R function you wrote before using the function “qqplot”.

```{r}
#repeat R function of Exercise 9.4
dLaplace=function(x){
  y=0.5*exp(-abs(x))
  return(y)
}
Metropolis=function(n,x0,sigma){
  x=numeric(n)
  x[1]=x0
  u=runif(n)
  k=0
  for (i in 2:n) {
    y=rnorm(1,x[i-1],sigma)
    if(u[i]<=(dLaplace(y)/dLaplace(x[i-1])))
      x[i]=y
      else
        x[i]=x[i-1]
  }
  return(x)
}

n=2e3
sigma=c(.05,.3,2,16)
x0=25
rwR1=Metropolis(n,x0,sigma[1])
rwR2=Metropolis(n,x0,sigma[2])
rwR3=Metropolis(n,x0,sigma[3])
rwR4=Metropolis(n,x0,sigma[4])


plot(rwR1,type = 'l',ylab = "x",xlab = "sigma=0.05")
plot(rwR2,type = 'l',ylab = "x",xlab = "sigma=0.3")
abline(h=2.5,lty = 1,col='blue')
abline(h=-2.5,lty = 1,col='blue')
plot(rwR3,type = 'l',ylab = "x",xlab = "sigma=2")
abline(h=2.5,lty = 1,col='blue')
abline(h=-2.5,lty = ,col='blue')
plot(rwR4,type = 'l',ylab = "x",xlab = "sigma=16")
abline(h=2.5,lty = 1,col='blue')
abline(h=-2.5,lty = 1,col='blue')

#Rcpp function for  Exercise 9.4

library(Rcpp)
Rcpp::sourceCpp('E:/tjjs/hw9/rw.cpp')

n=2e3
sigma=c(.05,.3,2,16)
x0=25
rwC1=Metropolis_c(n,x0,sigma[1])
rwC2=Metropolis_c(n,x0,sigma[2])
rwC3=Metropolis_c(n,x0,sigma[3])
rwC4=Metropolis_c(n,x0,sigma[4])


plot(rwC1,type = 'l',ylab = "x",xlab = "sigma=0.05")
plot(rwC2,type = 'l',ylab = "x",xlab = "sigma=0.5")
abline(h=2.5,lty = 1,col='blue')
abline(h=-2.5,lty = 1,col='blue')
plot(rwC3,type = 'l',ylab = "x",xlab = "sigma=2")
abline(h=2.5,lty = 1,col='blue')
abline(h=-2.5,lty = ,col='blue')
plot(rwC4,type = 'l',ylab = "x",xlab = "sigma=16")
abline(h=2.5,lty = 1,col='blue')
abline(h=-2.5,lty = 1,col='blue')

#qqplot

qqplot(rwR1[-(1:500)],rwC1[-(1:500)],xlab = "R",ylab = "Rcpp",main="qqplot for sigma=0.05")#Abandon burn-in period
qqplot(rwR2[-(1:500)],rwC2[-(1:500)],xlab = "R",ylab = "Rcpp",main="qqplot for sigma=0.3")
qqplot(rwR3[-(1:500)],rwC3[-(1:500)],xlab = "R",ylab = "Rcpp",main="qqplot for sigma=2")
qqplot(rwR4[-(1:500)],rwC4[-(1:500)],xlab = "R",ylab = "Rcpp",main="qqplot for sigma=16")
```

(2)Campare the computation time of the two functions with the function “microbenchmark”.


```{r}
library(microbenchmark)
ts=microbenchmark(rw1R=Metropolis(n,x0,sigma[1]),rw1C=Metropolis_c(n,x0,sigma[1]),rw2R=Metropolis(n,x0,sigma[2]),rw2C=Metropolis_c(n,x0,sigma[2]),rw3R=Metropolis(n,x0,sigma[3]),rw3C=Metropolis_c(n,x0,sigma[3]),rw4R=Metropolis(n,x0,sigma[4]),rw4C=Metropolis_c(n,x0,sigma[4]))
ts
```

(3)Comments your results.

1.When σ=2, the chain is mixing well and converging to the target distribution after a short burn-in period of about 500. In this situation, we can see from the qqplot that the chain generated by R function is closed to the chain generated by Rcpp function. In other situations, because the convergence of the chain is not good, there are some differences between the chain generated by two methods

2.In different variance, Rcpp method can significantly reduce the chain generation time. So using Rcpp method can improve computing efficiency.





